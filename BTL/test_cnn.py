import numpy as np
import sounddevice as sd
import librosa
import os
from datetime import datetime
from model_cnn import cnn_forward
import time

# =====================================================================
# C·∫§U H√åNH
# =====================================================================

# Audio config
SAMPLE_RATE = 16000
DURATION = 2.0  # ƒê·ªô d√†i l·ªánh voice command (gi√¢y)
WAKE_WORD_DURATION = 1.5  # ƒê·ªô d√†i wake word "hey siri"
CHANNELS = 1

# Spectrogram config (gi·ªëng v·ªõi training)
SPECTROGRAM_CONFIG = {
    'n_fft': 512,
    'hop_length': 256,
    'n_mels': 64,
    'fmin': 20,
    'fmax': 8000,
    'window': 'hann',
    'power': 2.0,
}
MAX_TIME_FRAMES = 128

# Model config
CHECKPOINT_DIR = 'checkpoint_cnn'
MODEL_RUN = None  # S·∫Ω t·ª± ƒë·ªông ch·ªçn run m·ªõi nh·∫•t
CHECKPOINT_FILE = 'final_model_cnn.npz'  # S·ª≠ d·ª•ng final model

# Wake word detection config
WAKE_WORD_THRESHOLD = 0.3  # Ng∆∞·ª°ng nƒÉng l∆∞·ª£ng ƒë·ªÉ ph√°t hi·ªán "hey siri"
SILENCE_THRESHOLD = 0.02  # Ng∆∞·ª°ng silence

# =====================================================================
# AUDIO PROCESSING FUNCTIONS
# =====================================================================

def audio_to_log_mel_spectrogram(audio, sr=SAMPLE_RATE, max_frames=MAX_TIME_FRAMES):
    """
    Chuy·ªÉn ƒë·ªïi audio array th√†nh Log-Mel Spectrogram
    
    Parameters:
    -----------
    audio : numpy.ndarray
        Audio signal
    sr : int
        Sample rate
    max_frames : int
        S·ªë l∆∞·ª£ng time frames t·ªëi ƒëa
        
    Returns:
    --------
    log_mel_spec : numpy.ndarray
        Log-Mel Spectrogram v·ªõi shape (n_mels, max_frames)
    """
    # T·∫°o Mel Spectrogram
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sr,
        n_fft=SPECTROGRAM_CONFIG['n_fft'],
        hop_length=SPECTROGRAM_CONFIG['hop_length'],
        n_mels=SPECTROGRAM_CONFIG['n_mels'],
        fmin=SPECTROGRAM_CONFIG['fmin'],
        fmax=SPECTROGRAM_CONFIG['fmax'],
        window=SPECTROGRAM_CONFIG['window'],
        power=SPECTROGRAM_CONFIG['power']
    )
    
    # Convert sang log scale (dB)
    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
    
    # Padding ho·∫∑c truncating ƒë·ªÉ c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh
    current_frames = log_mel_spec.shape[1]
    
    if current_frames < max_frames:
        # Padding v·ªõi gi√° tr·ªã nh·ªè nh·∫•t (silence)
        pad_width = max_frames - current_frames
        log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)), 
                               mode='constant', constant_values=-80)
    elif current_frames > max_frames:
        # Truncating
        log_mel_spec = log_mel_spec[:, :max_frames]
    
    return log_mel_spec

def detect_wake_word(audio, threshold=WAKE_WORD_THRESHOLD):
    """
    Ph√°t hi·ªán wake word "hey siri" d·ª±a tr√™n nƒÉng l∆∞·ª£ng √¢m thanh
    
    Parameters:
    -----------
    audio : numpy.ndarray
        Audio signal
    threshold : float
        Ng∆∞·ª°ng nƒÉng l∆∞·ª£ng ƒë·ªÉ ph√°t hi·ªán
        
    Returns:
    --------
    bool : True n·∫øu ph√°t hi·ªán wake word
    """
    # T√≠nh nƒÉng l∆∞·ª£ng (RMS - Root Mean Square)
    energy = np.sqrt(np.mean(audio**2))
    
    # Ki·ªÉm tra xem c√≥ v∆∞·ª£t ng∆∞·ª°ng kh√¥ng
    return energy > threshold

def record_audio(duration=DURATION, sample_rate=SAMPLE_RATE):
    """
    Thu √¢m t·ª´ microphone
    
    Parameters:
    -----------
    duration : float
        Th·ªùi gian ghi √¢m (gi√¢y)
    sample_rate : int
        Sample rate
        
    Returns:
    --------
    audio : numpy.ndarray
        Audio signal
    """
    print(f"üé§ ƒêang ghi √¢m trong {duration}s...")
    audio = sd.rec(int(duration * sample_rate), 
                   samplerate=sample_rate, 
                   channels=CHANNELS,
                   dtype='float32')
    sd.wait()
    print("‚úÖ Ho√†n t·∫•t ghi √¢m")
    
    return audio.flatten()

# =====================================================================
# MODEL FUNCTIONS
# =====================================================================

def load_model(checkpoint_path):
    """
    Load model parameters t·ª´ checkpoint
    
    Parameters:
    -----------
    checkpoint_path : str
        ƒê∆∞·ªùng d·∫´n ƒë·∫øn checkpoint file
        
    Returns:
    --------
    parameters : dict
        Model parameters
    """
    print(f"üì• ƒêang load model t·ª´: {checkpoint_path}")
    data = np.load(checkpoint_path)
    parameters = {}
    for key in data.files:
        parameters[key] = data[key]
    print("‚úÖ ƒê√£ load model th√†nh c√¥ng")
    return parameters

def load_label_mapping(run_dir):
    """
    Load label mapping t·ª´ run directory
    
    Parameters:
    -----------
    run_dir : str
        ƒê∆∞·ªùng d·∫´n ƒë·∫øn run directory
        
    Returns:
    --------
    label_mapping : dict
        Mapping t·ª´ class index -> command name
    """
    label_mapping_path = os.path.join(run_dir, 'label_mapping.npy')
    if os.path.exists(label_mapping_path):
        label_mapping = np.load(label_mapping_path, allow_pickle=True).item()
        print(f"‚úÖ ƒê√£ load label mapping: {len(label_mapping)} l·ªánh")
        return label_mapping
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y label_mapping.npy, s·ª≠ d·ª•ng mapping m·∫∑c ƒë·ªãnh")
        # Fallback: t·∫°o mapping m·∫∑c ƒë·ªãnh t·ª´ data_GK folder
        data_dir = 'D:/GitHub/Deep-Learning/BTL/data_GK'
        if os.path.exists(data_dir):
            command_folders = sorted([f for f in os.listdir(data_dir) 
                                    if os.path.isdir(os.path.join(data_dir, f))])
            label_mapping = {idx: cmd for idx, cmd in enumerate(command_folders)}
            return label_mapping
        else:
            raise ValueError("Kh√¥ng t√¨m th·∫•y label mapping!")

def find_latest_run(checkpoint_dir):
    """
    T√¨m run directory m·ªõi nh·∫•t
    
    Parameters:
    -----------
    checkpoint_dir : str
        ƒê∆∞·ªùng d·∫´n ƒë·∫øn checkpoint directory
        
    Returns:
    --------
    str : T√™n c·ªßa run directory m·ªõi nh·∫•t
    """
    if not os.path.exists(checkpoint_dir):
        return None
    
    runs = [d for d in os.listdir(checkpoint_dir) 
            if os.path.isdir(os.path.join(checkpoint_dir, d)) and d.startswith('run_')]
    
    if not runs:
        return None
    
    # S·∫Øp x·∫øp theo th·ªùi gian (d·ª±a tr√™n t√™n)
    runs.sort(reverse=True)
    return runs[0]

def predict_command(audio, parameters, label_mapping):
    """
    D·ª± ƒëo√°n l·ªánh t·ª´ audio
    
    Parameters:
    -----------
    audio : numpy.ndarray
        Audio signal
    parameters : dict
        Model parameters
    label_mapping : dict
        Label mapping
        
    Returns:
    --------
    command : str
        T√™n l·ªánh ƒë∆∞·ª£c d·ª± ƒëo√°n
    confidence : float
        ƒê·ªô tin c·∫≠y (probability)
    """
    # Chuy·ªÉn audio th√†nh spectrogram
    spectrogram = audio_to_log_mel_spectrogram(audio)
    
    # Reshape ƒë·ªÉ ph√π h·ª£p v·ªõi input c·ªßa CNN model
    # CNN expects: (N, C, H, W) where H=128, W=64
    # Resize spectrogram v·ªÅ 128x64 (gi·ªëng v·ªõi training)
    import cv2
    spectrogram_resized = cv2.resize(spectrogram, (64, 128), interpolation=cv2.INTER_AREA)
    
    # Reshape: (1, 1, 128, 64) - batch_size=1, channels=1
    X = spectrogram_resized.reshape(1, 1, 128, 64)
    
    # Forward pass
    Z, _ = cnn_forward(X, parameters, training=False)
    
    # Softmax ƒë·ªÉ t√≠nh probability
    Z_shift = Z - np.max(Z, axis=1, keepdims=True)
    exp_Z = np.exp(Z_shift)
    probs = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    
    # L·∫•y prediction
    pred_idx = np.argmax(probs, axis=1)[0]
    confidence = probs[0, pred_idx]
    
    command = label_mapping.get(pred_idx, f"Unknown_{pred_idx}")
    
    return command, confidence

# =====================================================================
# MAIN TESTING LOOP
# =====================================================================

def continuous_voice_command_recognition():
    """
    Ch·∫°y continuous voice command recognition v·ªõi wake word detection
    """
    print("=" * 70)
    print("üéØ VOICE COMMAND RECOGNITION SYSTEM (CNN)")
    print("=" * 70)
    
    # T√¨m run m·ªõi nh·∫•t
    run_name = MODEL_RUN or find_latest_run(CHECKPOINT_DIR)
    
    if not run_name:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y run n√†o trong {CHECKPOINT_DIR}")
        return
    
    run_dir = os.path.join(CHECKPOINT_DIR, run_name)
    checkpoint_path = os.path.join(run_dir, CHECKPOINT_FILE)
    
    if not os.path.exists(checkpoint_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint: {checkpoint_path}")
        print("\nüìã Available checkpoints:")
        if os.path.exists(run_dir):
            for f in os.listdir(run_dir):
                if f.endswith('.npz'):
                    print(f"   - {f}")
        return
    
    # Load model
    parameters = load_model(checkpoint_path)
    label_mapping = load_label_mapping(run_dir)
    
    print(f"\nüìÅ S·ª≠ d·ª•ng run: {run_name}")
    print("\nüìã C√°c l·ªánh c√≥ th·ªÉ nh·∫≠n di·ªán:")
    for idx, cmd in sorted(label_mapping.items()):
        print(f"   {idx}: {cmd}")
    
    print("\n" + "=" * 70)
    print("üéôÔ∏è  B·∫ÆT ƒê·∫¶U NH·∫¨N DI·ªÜN GI·ªåNG N√ìI")
    print("=" * 70)
    print("üí° H∆∞·ªõng d·∫´n:")
    print("   1. N√≥i 'Hey Siri' ƒë·ªÉ k√≠ch ho·∫°t")
    print("   2. Sau khi nghe beep, n√≥i l·ªánh trong 2 gi√¢y")
    print("   3. Nh·∫•n Ctrl+C ƒë·ªÉ tho√°t")
    print("=" * 70)
    
    try:
        while True:
            print("\nüëÇ ƒêang l·∫Øng nghe 'Hey Siri'...")
            
            # Thu √¢m li√™n t·ª•c ƒë·ªÉ ph√°t hi·ªán wake word
            wake_audio = record_audio(duration=WAKE_WORD_DURATION, sample_rate=SAMPLE_RATE)
            
            # Ki·ªÉm tra wake word
            if detect_wake_word(wake_audio):
                print("üîî ƒê√£ ph√°t hi·ªán wake word! S·∫µn s√†ng nh·∫≠n l·ªánh...")
                
                # Ph√°t beep sound (optional)
                try:
                    # T·∫°o beep sound ng·∫Øn
                    beep_duration = 0.1
                    beep_freq = 800
                    t = np.linspace(0, beep_duration, int(SAMPLE_RATE * beep_duration))
                    beep = 0.3 * np.sin(2 * np.pi * beep_freq * t)
                    sd.play(beep, SAMPLE_RATE)
                    sd.wait()
                except:
                    pass
                
                # Thu √¢m l·ªánh
                command_audio = record_audio(duration=DURATION, sample_rate=SAMPLE_RATE)
                
                # D·ª± ƒëo√°n l·ªánh
                print("üîç ƒêang x·ª≠ l√Ω...")
                command, confidence = predict_command(command_audio, parameters, label_mapping)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                print("\n" + "=" * 70)
                print(f"‚ú® L·ªÜNH: {command}")
                print(f"üìä ƒê·ªô tin c·∫≠y: {confidence*100:.2f}%")
                print("=" * 70)
                
                # Th√™m delay tr∆∞·ªõc khi l·∫Øng nghe ti·∫øp
                time.sleep(1)
            else:
                print("   (Ch∆∞a nghe th·∫•y 'Hey Siri', th·ª≠ l·∫°i...)")
                time.sleep(0.5)
                
    except KeyboardInterrupt:
        print("\n\nüëã ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh")
        print("=" * 70)

def single_test():
    """
    Test m·ªôt l·∫ßn duy nh·∫•t (kh√¥ng c·∫ßn wake word)
    """
    print("=" * 70)
    print("üéØ VOICE COMMAND RECOGNITION - SINGLE TEST (CNN)")
    print("=" * 70)
    
    # T√¨m run m·ªõi nh·∫•t
    run_name = MODEL_RUN or find_latest_run(CHECKPOINT_DIR)
    
    if not run_name:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y run n√†o trong {CHECKPOINT_DIR}")
        return
    
    run_dir = os.path.join(CHECKPOINT_DIR, run_name)
    checkpoint_path = os.path.join(run_dir, CHECKPOINT_FILE)
    
    if not os.path.exists(checkpoint_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint: {checkpoint_path}")
        return
    
    # Load model
    parameters = load_model(checkpoint_path)
    label_mapping = load_label_mapping(run_dir)
    
    print(f"\nüìÅ S·ª≠ d·ª•ng run: {run_name}")
    print("\nüìã C√°c l·ªánh c√≥ th·ªÉ nh·∫≠n di·ªán:")
    for idx, cmd in sorted(label_mapping.items()):
        print(f"   {idx}: {cmd}")
    
    print("\n" + "=" * 70)
    
    # Thu √¢m
    command_audio = record_audio(duration=DURATION, sample_rate=SAMPLE_RATE)
    
    # D·ª± ƒëo√°n
    print("üîç ƒêang x·ª≠ l√Ω...")
    command, confidence = predict_command(command_audio, parameters, label_mapping)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print("\n" + "=" * 70)
    print(f"‚ú® L·ªÜNH: {command}")
    print(f"üìä ƒê·ªô tin c·∫≠y: {confidence*100:.2f}%")
    print("=" * 70)

# =====================================================================
# MAIN
# =====================================================================

if __name__ == "__main__":
    import sys
    
    print("\nüé§ Voice Command Recognition System (CNN)")
    print("=" * 70)
    print("Ch·ªçn ch·∫ø ƒë·ªô:")
    print("  1. Continuous mode (v·ªõi wake word 'Hey Siri')")
    print("  2. Single test (test 1 l·∫ßn, kh√¥ng c·∫ßn wake word)")
    print("=" * 70)
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1/2): ").strip()
    
    if choice == "1":
        continuous_voice_command_recognition()
    elif choice == "2":
        single_test()
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
